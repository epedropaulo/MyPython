{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Pedro Paulo Dantas Silva Martins\n",
    "\n",
    "Lista 2, respostas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - (Kernels em todo canto)**\n",
    "\n",
    "a) Sabemos que o $\\beta_0$ captura justamente o deslocamento dos nossos dados, caso os dados não fossem deslocados (com relação a origem), o nosso $\\beta_0$ seria 0.<br>\n",
    "E o ato de tirarmos a média de todos os dados, é justamente o processo que centraliza os dados com relação a origem. Logo quando fazemos isso o $\\beta_0 = 0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) $\\hat{\\beta} = argmin_{\\tilde{\\beta}} \\sum_{i=1}^n (y_i - \\sum_{j=1}^p x_{ij} \\tilde{\\beta_j})^2 + \\lambda \\sum_{j=1}^p \\tilde{\\beta_j}^2$<br>\n",
    "É possível perceber que nesse caso não há uso de matrizes e vetores. A conta é feita pelas entradas. Transformarei para a forma vetorial e matricial.\n",
    "\n",
    "\n",
    "$\\hat{\\beta} = argmin_{\\tilde{\\beta}} (Y - X \\tilde{\\beta})^T (Y - X \\tilde{\\beta}) + \\lambda I \\tilde{\\beta}^T \\tilde{\\beta}$ Sabemos que o problema é convexo. Logo, se derivarmos em relação ao $\\tilde{\\beta}$ e igualarmos a 0 será o ponto de mínimo global, e portanto acharemos o nosso $\\hat{\\beta}$\n",
    "\n",
    "$-2X^T (Y - X \\hat{\\beta}) + 2\\lambda I \\hat{\\beta} = 0$ &harr; $-X^T Y + X^T X \\hat{\\beta} + \\lambda I \\hat{\\beta} = 0$ &harr; $(X^T X + \\lambda I\n",
    "\\hat{\\beta} = X^T Y$ &harr; $\\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T Y$\n",
    "\n",
    "É possível perceber que realmente quando $\\lambda = 0$ temos que o nosso $\\hat{\\beta}$ é igual ao de uma regressão linear."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) $\\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T Y$ Usando a fórmula sugerida pelo enunciado temos:<br>\n",
    "$\\hat{\\beta} = (\\frac{1}{\\lambda} I - \\frac{1}{\\lambda} I X^T (I + X \\frac{1}{\\lambda} I X^T)^{-1}X \\frac{1}{\\lambda} I) X^T Y$<br>\n",
    "$\\hat{\\beta} = \\frac{1}{\\lambda}IX^TY - \\frac{1}{\\lambda} I X^T(I + X\\frac{1}{\\lambda}I X^T)^{-1} X \\frac{1}{\\lambda} I X^T Y$ (Obs.: Perceba que $\\frac{1}{\\lambda} I X^T = X^T \\frac{1}{\\lambda} I$. Logo botamos o $X^T$ em evidência.)<br>\n",
    "$\\hat{\\beta} = X^T(\\frac{1}{\\lambda} I Y - \\frac{1}{\\lambda} I (I + X \\frac{1}{\\lambda} I X^T)^{-1} X \\frac{1}{\\lambda} I X^T Y)$; Se definirmos nosso $\\alpha = \\frac{1}{\\lambda} I Y - \\frac{1}{\\lambda} I (I + X \\frac{1}{\\lambda} I X^T)^{-1} X \\frac{1}{\\lambda} I X^T Y$<br>\n",
    "$\\hat{\\beta} = X^T \\alpha$ &harr; $\\hat{\\beta} = \\sum_{i=1}^{n} \\alpha_i x_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Sabemos que uma previsão é feita $y_* = \\beta x_*$ ($\\beta_0 = 0$). Portanto a previsão seria:<br>\n",
    "$y_* = \\sum_{i=1}^{n} \\alpha_i x_i x_*$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d6b32d6802feadfc7b2884ae209e4f61f5356a9fc8071acad924e2215544356"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
